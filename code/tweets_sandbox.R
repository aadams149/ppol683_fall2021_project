#Unfortunately I didn't end up using the values generated by this script
#in my app or broader analysis due to lack of time. Maybe I'll revisit this
#in the future.


#Read in libraries
library(tidyverse)
library(vroom)

# Read in previous county data ----------------------------------------

df = vroom::vroom('data/raw/counties_with_tweets.csv') %>%
  mutate(fips = 
           as.character(fips))


# Find most recent file of scraped tweets and read that in ------------

input_stem <- 'data/raw/most_recent_tweets/'
input_name <- 'most_recent_tweets_'

## Look for all files in the inputs folder that start with Primary_Scraping_input
list_of_input_files <- list.files(path = input_stem,pattern = input_name)

## Strip everything but the dates in the file names, and store the date as a vector of type date, sorted from greatest to least
input_file_dates <- substr(list_of_input_files,20,29) %>% as.Date(format="%Y-%m-%d") %>% sort(decreasing = TRUE)

## Take the first item in the sorted vector(The most recent input file date) and convert back to a string
latest_date <- input_file_dates[1] %>% as.character(format="%Y-%m-%d")

## Combine the resulting date as a string with the rest of the path for the input csv. 
input_path <- paste0(input_stem,input_name,latest_date,".csv") 

## Read in the csv 
tweets = vroom::vroom(input_path)


# EDA w/ tweets -------------------------------------------------------

#Note: Tweets were most recently scraped on 2021-12-10

tweets_per = 
  data.frame(table(tweets$username)) %>%
  rename('total_tweets' = 'Freq')

tweets_per_COVID = 
  data.frame(table(tweets[tweets$date >= '2020-03-10',]$username)) %>%
  rename('tweets_COVID' = 'Freq')

tweets_per_last60 = 
  data.frame(table(tweets[tweets$date >= '2021-10-10',]$username)) %>%
  rename('tweets_last60' = 'Freq')

tweet_counts = 
  tweets_per %>%
  left_join(
    tweets_per_COVID,
    by = 'Var1'
  ) %>%
  left_join(
    tweets_per_last60,
    by = 'Var1'
  )


# Combine all tweet summary tables ------------------------------------


all_accounts <-
  df %>%
  select(twitter) %>%
  left_join(
    tweet_counts,
    by = c('twitter' = 'Var1')
  ) %>%
  drop_na(
    twitter
  ) %>%
  distinct()

#Replace missing values with 0
all_accounts[is.na(all_accounts)] <- 0


# Calculate mean likes/retweets/replies -------------------------------
engagement = data.frame()
mean_likes = c()
mean_retweets = c()
mean_replies = c()
for (ii in all_accounts$twitter){
  select_tweets <-
    tweets %>%
    filter(username == ii)
  
  if(length(select_tweets) == 0){
    mean_likes <- 
      append(mean_likes, 0)
    mean_retweets <- 
      append(mean_retweets, 0)
    mean_replies <- 
      append(mean_replies, 0)
  }
  else{
    mean_likes <-
      append(mean_likes, mean(select_tweets$likes_count))
    mean_retweets <-
      append(mean_retweets, mean(select_tweets$retweets_count))
    mean_replies <-
      append(mean_replies, mean(select_tweets$replies_count))
  }
}
#Bind together into data frame
engagement <-
  cbind.data.frame(
    all_accounts$twitter,
    mean_likes,
    mean_retweets,
    mean_replies
  ) 

engagement <-
  engagement %>%
  rename('twitter' = `all_accounts$twitter`)

#Replace missing values with 0
engagement[is.na(engagement)] <- 0

df <-
  df %>%
  left_join(
    all_accounts,
    by = 'twitter'
  ) %>%
  left_join(
    engagement,
    by = 'twitter'
  )

#write_csv(df, 'data/raw/counties_with_tweets1.csv')
